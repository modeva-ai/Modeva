{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Wrapping Scored Classifier\n\nThis example requires full licence, and the program will break if you use the trial licence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To install the required package, use the following command:\n# !pip install modeva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Authentication\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To get authentication, use the following command: (To get full access please replace the token to your own token)\n# from modeva.utils.authenticate import authenticate\n# authenticate(token='eaaa4301-b140-484c-8e93-f9f633c8bacb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import modeva modules\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nfrom modeva import DataSet\nfrom modeva import TestSuite\nfrom modeva.models import MoXGBClassifier\nfrom modeva.models import MoScoredClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build a model and save the prediction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(n_samples=10000, n_features=2,\n                           n_redundant=0, random_state=42)\nX_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n    X, y, np.arange((len(X))), test_size=0.2, random_state=42)\n\nmodel1 = MoXGBClassifier(max_depth=1)\nmodel1.fit(X_train, y_train)\nproba1 = model1.predict_proba(X)[:, 1]\n\nmodel2 = MoXGBClassifier(max_depth=2)\nmodel2.fit(X_train, y_train)\nproba2 = model2.predict_proba(X)[:, 1]\n\ndata = pd.DataFrame(np.concatenate([X, y.reshape(-1, 1),\n                                    proba1.reshape(-1, 1), proba2.reshape(-1, 1)], 1),\n                    columns=['X' + str(i) for i in range(X.shape[1])] + ['Y', \"proba_XGB1\", \"proba_XGB2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrap the data into Modeva\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds = DataSet(name=\"scored-test-demo\")\nds.load_dataframe(data)\nds.set_train_idx(train_idx=train_indices)\nds.set_test_idx(test_idx=test_indices)\nds.set_target(feature=\"Y\")\nds.set_inactive_features((\"proba_XGB1\", \"proba_XGB2\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert the model into Modeva\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scored_model1 = MoScoredClassifier(dataset=ds, prediction_proba_name=\"proba_XGB1\")\nscored_model2 = MoScoredClassifier(dataset=ds, prediction_proba_name=\"proba_XGB2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create test suite for diagnostics\nNote that the robustness test is not available for scored model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ts = TestSuite(ds, scored_model1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run accuracy test without the model object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = ts.diagnose_accuracy_table()\nresults.table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run residual analysis test without the model object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = ts.diagnose_residual_analysis(features=\"X1\")\nresults.table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run reliability test without the model object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = ts.diagnose_reliability()\nresults.table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run resilience test without the model object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = ts.diagnose_resilience()\nresults.table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run slicing accuracy test without the model object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = ts.diagnose_slicing_accuracy(features=\"X1\",\n                                       dataset=\"main\",\n                                       metric=\"AUC\",\n                                       threshold=0)\nresults.table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run slicing overfit test without the model object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = ts.diagnose_slicing_overfit(features=\"X1\",\n                                      train_dataset=\"train\",\n                                      test_dataset=\"test\",\n                                      metric=\"LogLoss\")\nresults.table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare two scored models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tsc = TestSuite(ds, models=[scored_model1, scored_model2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run accuracy test without the model object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = tsc.compare_accuracy_table()\nresults.table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run slicing accuracy test without the model object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = tsc.compare_slicing_accuracy(features=\"X1\",\n                                       dataset=\"test\",\n                                       metric=\"AUC\")\nresults.table"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}